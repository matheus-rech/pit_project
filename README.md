# PitVQA Surgical Workflow - Publication Package

**Clean, organized repository for publication, reproduction, and audit.**

ğŸ”— **GitHub Repository**: https://github.com/matheus-rech/pit_project
ğŸ“Š **Dataset**: https://huggingface.co/datasets/mmrech/pitvqa-comprehensive-spatial
ğŸ¤– **Model**: https://huggingface.co/mmrech/pitvqa-qwen2vl-spatial

---

## ğŸ“ Directory Structure

```
publication_ready/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                   # Git ignore patterns
â”œâ”€â”€ MANIFEST.json                # Project manifest
â”‚
â”œâ”€â”€ gradio_demo.py               # ğŸ¬ Interactive Gradio demo
â”œâ”€â”€ GRADIO_DEMO_README.md        # Demo setup guide
â”œâ”€â”€ test_gradio_dependencies.py  # Dependency checker
â”œâ”€â”€ CORRECTED_VALIDATION_REPORT.json  # Proper metrics (100% data, 80% model)
â”‚
â”œâ”€â”€ scripts/                     # Core Python scripts
â”‚   â”œâ”€â”€ train_unified_vlm.py
â”‚   â”œâ”€â”€ evaluate_unified_vlm.py
â”‚   â”œâ”€â”€ create_comprehensive_spatial_dataset.py
â”‚   â”œâ”€â”€ validate_dataset_integrity.py
â”‚   â””â”€â”€ publication_validation_report.py
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter/Colab notebooks
â”‚   â”œâ”€â”€ train_spatial_qwen2vl_colab.ipynb
â”‚   â”œâ”€â”€ train_and_demo_colab.ipynb
â”‚   â””â”€â”€ 01_upload_pitvqa_to_huggingface.ipynb
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ COLAB_TRAINING_GUIDE.md
â”‚   â”œâ”€â”€ VIDEO_DEMO_GUIDE.md
â”‚   â””â”€â”€ SPATIAL_TRAINING_PLAN.md
â”‚
â”œâ”€â”€ validation/                  # Validation reports
â”‚   â”œâ”€â”€ final_validation_report.json
â”‚   â”œâ”€â”€ publication_validation_report.json
â”‚   â””â”€â”€ dataset_integrity_report.json
â”‚
â”œâ”€â”€ data/                        # Data references (not actual data)
â”‚   â”œâ”€â”€ DATA_SOURCES.md          # Links to HuggingFace datasets
â”‚   â””â”€â”€ GROUND_TRUTH_README.md
â”‚
â”œâ”€â”€ models/                      # Model references (not actual models)
â”‚   â””â”€â”€ MODELS.md                # Links to HuggingFace models
â”‚
â””â”€â”€ validation/                  # Validation reports
    â”œâ”€â”€ final_validation_report.json
    â”œâ”€â”€ publication_validation_report.json
    â””â”€â”€ dataset_integrity_report.json
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Train Model (Colab)

```bash
# Upload to Google Colab:
notebooks/train_and_demo_colab.ipynb

# Or use the training-only notebook:
notebooks/train_spatial_qwen2vl_colab.ipynb
```

### 3. Validate Results

```bash
python scripts/validate_dataset_integrity.py
```

---

## ğŸ“Š Dataset

**Primary:** `mmrech/pitvqa-comprehensive-spatial` (HuggingFace)
- 9,125 training samples
- 1,014 validation samples
- 100% ground truth accuracy (validated)

See `data/DATA_SOURCES.md` for details.

---

## ğŸ¤– Models

**Baseline:** `mmrech/pitvqa-qwen2vl-unified`
**Publication:** `mmrech/pitvqa-qwen2vl-spatial` (train with notebook)

See `models/MODELS.md` for details.

---

## ğŸ“– Documentation

| Guide | Purpose |
|-------|---------|
| `docs/COLAB_TRAINING_GUIDE.md` | Step-by-step Colab training |
| `docs/VIDEO_DEMO_GUIDE.md` | Creating demo videos |
| `docs/SPATIAL_TRAINING_PLAN.md` | Complete training roadmap |

---

## âœ… Validation

All validation reports in `validation/`:
- Dataset integrity: 100% accuracy
- No AI hallucinations: 0%
- Ground truth fidelity: Perfect

---

## ğŸ“ Citation

```bibtex
@article{yourname2026pitvqa,
  title={PitVQA: Multi-Task Vision-Language Model for Pituitary Surgery},
  author={Your Name and Collaborators},
  journal={Medical Image Analysis},
  year={2026}
}
```

---

## ğŸ“§ Contact

For questions: your.email@institution.edu

---

**Generated:** 2026-01-14 10:58:19
**Organization script:** `organize_for_publication.py`
