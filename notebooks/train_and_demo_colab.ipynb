{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf PitVQA Training + Video Demo Pipeline",
    "",
    "**Complete Pipeline:** Train \u2192 Inference \u2192 Demo Video",
    "",
    "This notebook:",
    "1. Trains spatial fine-tuning (2-6 hours)",
    "2. Runs inference on surgical frames",
    "3. Creates annotated video with bounding boxes and labels",
    "4. Generates comparison visualization",
    "",
    "**Hardware:** T4 GPU (Free) or A100 (Pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q transformers accelerate peft trl datasets bitsandbytes qwen-vl-utils pillow opencv-python imageio imageio-ffmpeg matplotlib\nprint(\"\u2705 Installed!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from huggingface_hub import notebook_login\nnotebook_login()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Load Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\ndataset = load_dataset(\"mmrech/pitvqa-comprehensive-spatial\")\nprint(f\"Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Training (Use Full Notebook)",
    "",
    "**For complete training cells, use:** `train_spatial_qwen2vl_colab.ipynb`",
    "",
    "Or continue with pre-trained model: `mmrech/pitvqa-qwen2vl-spatial`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "# \ud83c\udfac DEMO PIPELINE",
    "## 4\ufe0f\u20e3 Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom peft import PeftModel\nimport torch\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nmodel = PeftModel.from_pretrained(model, \"mmrech/pitvqa-qwen2vl-spatial\")\nprocessor = AutoProcessor.from_pretrained(\"mmrech/pitvqa-qwen2vl-spatial\", trust_remote_code=True)\nprint(\"\u2705 Model loaded!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 Extract Frames for Demo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get 30 validation frames\ndemo_frames = []\ndemo_metadata = []\nfor i in range(30):\n    sample = dataset['validation'][i]\n    demo_frames.append(sample['image'])\n    demo_metadata.append(sample)\nprint(f\"\u2705 Extracted {len(demo_frames)} frames\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\nfrom tqdm import tqdm\n\ndef extract_points(text):\n    pattern = r\"<point x='([\\d.]+)' y='([\\d.]+)'>([^<]+)</point>\"\n    return [{'x': float(m[0]), 'y': float(m[1]), 'label': m[2]} \n            for m in re.findall(pattern, text)]\n\npredictions = []\nfor frame, meta in tqdm(zip(demo_frames, demo_metadata), total=len(demo_frames)):\n    question = \"Point to all surgical instruments visible.\"\n    conv = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question}]}]\n    text = processor.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n    inputs = processor(text=[text], images=[frame], return_tensors=\"pt\").to(model.device)\n    \n    with torch.inference_mode():\n        outputs = model.generate(**inputs, max_new_tokens=200)\n    response = processor.decode(outputs[0], skip_special_tokens=True)\n    \n    predictions.append({\n        'image': frame,\n        'points': extract_points(response),\n        'response': response\n    })\n\nprint(f\"\u2705 Inference done! {sum(len(p['points']) for p in predictions)} detections\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\ufe0f\u20e3 Create Annotated Video"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import cv2\nimport numpy as np\nfrom PIL import Image\n\ndef draw_boxes(image, points):\n    img = np.array(image.convert('RGB'))\n    h, w = img.shape[:2]\n    \n    for p in points:\n        x_px = int(p['x'] * w / 100)\n        y_px = int(p['y'] * h / 100)\n        \n        # Draw box\n        box_size = 40\n        color = (0, 255, 0)  # Green\n        cv2.rectangle(img, \n                     (x_px-box_size, y_px-box_size),\n                     (x_px+box_size, y_px+box_size),\n                     color, 2)\n        \n        # Draw point\n        cv2.circle(img, (x_px, y_px), 5, color, -1)\n        \n        # Add label\n        cv2.putText(img, p['label'], (x_px-box_size, y_px-box_size-10),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n    \n    return img\n\nannotated = [draw_boxes(p['image'], p['points']) for p in tqdm(predictions)]\nprint(f\"\u2705 Created {len(annotated)} annotated frames\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\ufe0f\u20e3 Export Video"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import imageio\noutput_video = \"pitvqa_demo.mp4\"\nimageio.mimsave(output_video, annotated, fps=2, codec='libx264', quality=8)\nprint(f\"\u2705 Video saved: {output_video}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\ufe0f\u20e3 Create Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "comparison = []\nfor i, pred in enumerate(predictions):\n    orig = np.array(pred['image'].convert('RGB'))\n    annot = annotated[i]\n    \n    h = min(orig.shape[0], annot.shape[0])\n    orig_r = cv2.resize(orig, (int(orig.shape[1]*h/orig.shape[0]), h))\n    annot_r = cv2.resize(annot, (int(annot.shape[1]*h/annot.shape[0]), h))\n    \n    side_by_side = np.hstack([orig_r, annot_r])\n    cv2.putText(side_by_side, \"Original\", (20,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n    cv2.putText(side_by_side, \"With Detection\", (orig_r.shape[1]+20,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n    comparison.append(side_by_side)\n\nimageio.mimsave(\"pitvqa_comparison.mp4\", comparison, fps=2, codec='libx264', quality=8)\nprint(\"\u2705 Comparison video created!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf81 Download Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import files\n!zip pitvqa_demo.zip pitvqa_demo.mp4 pitvqa_comparison.mp4\nfiles.download('pitvqa_demo.zip')\nprint(\"\u2705 Downloading!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2705 Complete!",
    "",
    "**You now have:**",
    "1. pitvqa_demo.mp4 - Annotated surgical video",
    "2. pitvqa_comparison.mp4 - Side-by-side comparison",
    "3. Trained model: mmrech/pitvqa-qwen2vl-spatial",
    "",
    "**Use for:** Papers, presentations, demos"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}