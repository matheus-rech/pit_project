{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Spatial Fine-Tuning for PitVQA Surgical VLM\n",
    "\n",
    "**Stage 1: Coordinate-Aware Training**\n",
    "\n",
    "This notebook fine-tunes `mmrech/pitvqa-qwen2vl-unified` on spatial localization data to enable accurate instrument pointing.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Dataset\n",
    "- **Name:** `mmrech/pitvqa-comprehensive-spatial`\n",
    "- **Samples:** 9,125 train / 1,014 validation\n",
    "- **Validated:** 100% ground truth accuracy\n",
    "- **Content:** Instruments + Anatomy with precise x,y coordinates\n",
    "\n",
    "## üéØ Expected Results\n",
    "- Coordinate MAE < 15%\n",
    "- Quadrant Accuracy > 80%\n",
    "- Instrument F1 > 0.80\n",
    "\n",
    "---\n",
    "\n",
    "**Hardware:** T4 GPU (Colab Free) or A100 (Colab Pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers==4.45.0 \\\n",
    "    accelerate==0.34.0 \\\n",
    "    peft==0.13.0 \\\n",
    "    trl==0.11.0 \\\n",
    "    datasets==2.21.0 \\\n",
    "    bitsandbytes==0.44.0 \\\n",
    "    qwen-vl-utils==0.0.8 \\\n",
    "    pillow==10.4.0\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace login (for uploading model)\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading mmrech/pitvqa-comprehensive-spatial...\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"mmrech/pitvqa-comprehensive-spatial\")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded!\")\n",
    "print(f\"   Train: {len(dataset['train']):,} samples\")\n",
    "print(f\"   Validation: {len(dataset['validation']):,} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample:\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"   Messages: {sample['messages']}\")\n",
    "print(f\"   Metadata: {sample['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "print(\"Loading Qwen2-VL-2B-Instruct...\")\n",
    "\n",
    "# Quantization config (for memory efficiency)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load your existing adapter\n",
    "print(\"Loading mmrech/pitvqa-qwen2vl-unified adapter...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"mmrech/pitvqa-qwen2vl-unified\",\n",
    "    is_trainable=True,\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "print(f\"   Base: Qwen2-VL-2B-Instruct\")\n",
    "print(f\"   Adapter: pitvqa-qwen2vl-unified\")\n",
    "print(f\"   Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Configure LoRA for Spatial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add NEW LoRA adapters for spatial reasoning\n",
    "# (Lower rank than initial training for fine-grained tuning)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Reduced from 32 for fine-tuning\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Add adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úÖ LoRA adapters configured!\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import io\n",
    "import json\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Convert dataset samples to Qwen2-VL format.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for i in range(len(examples['messages'])):\n",
    "        # Parse messages (stored as string)\n",
    "        messages = json.loads(examples['messages'][i]) if isinstance(examples['messages'][i], str) else examples['messages'][i]\n",
    "        \n",
    "        # Format for Qwen2-VL\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": messages[0]['content'].replace('<image>\\n', '')}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": messages[1]['content']}]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = processor.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "        \n",
    "        # Get image\n",
    "        image = examples['image'][i]\n",
    "        if isinstance(image, dict) and 'bytes' in image:\n",
    "            image = Image.open(io.BytesIO(image['bytes']))\n",
    "        images.append(image)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "print(\"‚úÖ Preprocessing function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pitvqa-qwen2vl-spatial\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Increase if you have more VRAM\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,  # Effective batch size = 16\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",  # Change to \"tensorboard\" if you want logging\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration ready!\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size} (effective: {training_args.gradient_accumulation_steps})\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Total steps: ~{(len(dataset['train']) // training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=processor.tokenizer,\n",
    "    formatting_func=preprocess_function,\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")\n",
    "print(\"\\nüöÄ Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"   Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "output_dir = \"./pitvqa-qwen2vl-spatial-final\"\n",
    "trainer.save_model(output_dir)\n",
    "processor.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {output_dir}\")\n",
    "\n",
    "# Push to HuggingFace Hub\n",
    "model.push_to_hub(\n",
    "    \"mmrech/pitvqa-qwen2vl-spatial\",\n",
    "    commit_message=\"Stage 1: Spatial fine-tuning on comprehensive-spatial dataset\"\n",
    ")\n",
    "processor.push_to_hub(\"mmrech/pitvqa-qwen2vl-spatial\")\n",
    "\n",
    "print(\"\\n‚úÖ Model pushed to HuggingFace Hub!\")\n",
    "print(\"   üîó https://huggingface.co/mmrech/pitvqa-qwen2vl-spatial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a validation sample\n",
    "test_sample = dataset['validation'][0]\n",
    "\n",
    "# Prepare input\n",
    "messages = json.loads(test_sample['messages']) if isinstance(test_sample['messages'], str) else test_sample['messages']\n",
    "question = messages[0]['content'].replace('<image>\\n', '')\n",
    "ground_truth = messages[1]['content']\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=[text], images=[test_sample['image']], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"üìù TEST SAMPLE\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground Truth: {ground_truth}\")\n",
    "print(f\"Model Output: {response.split('assistant')[-1].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Training Complete!\n",
    "\n",
    "Your spatial fine-tuned model is now available at:\n",
    "- **HuggingFace:** `mmrech/pitvqa-qwen2vl-spatial`\n",
    "- **Local:** `./pitvqa-qwen2vl-spatial-final`\n",
    "\n",
    "### Next Steps:\n",
    "1. Run evaluation notebook to measure coordinate accuracy\n",
    "2. Test on real surgical frames\n",
    "3. (Optional) Stage 2: Add video/temporal training"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
