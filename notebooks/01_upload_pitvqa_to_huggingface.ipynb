{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 Open in Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matheus-rech/pitvqa-surgical-workflow/blob/main/notebooks/01_upload_pitvqa_to_huggingface.ipynb)\n",
        "\n",
        "**Quick Start:** Click the badge above to open this notebook directly in Google Colab!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfe5 PitVQA Dataset \u2192 HuggingFace Hub\n",
        "\n",
        "This notebook downloads the PitVQA surgical VQA dataset from UCL Research Data Repository\n",
        "and uploads it to HuggingFace Hub for easy access during training.\n",
        "\n",
        "**What this does:**\n",
        "1. Downloads PitVQA dataset (~7.56 GB) using Colab's disk\n",
        "2. Extracts and organizes the data\n",
        "3. Creates a proper HuggingFace Dataset\n",
        "4. Uploads to your HuggingFace account\n",
        "\n",
        "**Requirements:**\n",
        "- HuggingFace account with write token\n",
        "- ~15GB free disk space (Colab provides ~100GB)\n",
        "\n",
        "**Time estimate:** 30-60 minutes (mostly download time)\n",
        "\n",
        "---\n",
        "**Project:** PitVQA Surgical Workflow Understanding  \n",
        "**Target:** MICCAI 2026  \n",
        "**Author:** Generated with Claude Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install -q huggingface_hub datasets pillow tqdm pandas ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcbe Disk space check:\n",
            "   Total: 460 GB\n",
            "   Used: 438 GB\n",
            "   Free: 21 GB\n",
            "\n",
            "\u2705 You need ~15GB free space.\n"
          ]
        }
      ],
      "source": [
        "# Check available disk space\n",
        "try:\n",
        "    import google.colab\n",
        "    import subprocess\n",
        "    result = subprocess.run(['df', '-h', '/content'], capture_output=True, text=True)\n",
        "    print(result.stdout)\n",
        "    print(\"\\n\u2705 You need ~15GB free. Colab typically provides ~100GB.\")\n",
        "except ImportError:\n",
        "    import shutil\n",
        "    import os\n",
        "    total, used, free = shutil.disk_usage(os.getcwd())\n",
        "    print(f\"\ud83d\udcbe Disk space check:\")\n",
        "    print(f\"   Total: {total // (1024**3)} GB\")\n",
        "    print(f\"   Used: {used // (1024**3)} GB\")\n",
        "    print(f\"   Free: {free // (1024**3)} GB\")\n",
        "    print(\"\\n\u2705 You need ~15GB free space.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. HuggingFace Authentication\n",
        "\n",
        "Enter your HuggingFace token with **write** permissions.  \n",
        "Get one at: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Logged in to HuggingFace!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login",
        "",
        "# Login to HuggingFace (will prompt for token)",
        "notebook_login()",
        "",
        "print(\"\u2705 Logged in to HuggingFace!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udce6 Dataset will be uploaded to: https://huggingface.co/datasets/matheus-rech/pitvqa-surgical\n"
          ]
        }
      ],
      "source": [
        "# Configure your dataset repository\n",
        "HF_USERNAME = \"matheus-rech\"  # Your HuggingFace username\n",
        "DATASET_NAME = \"pitvqa-surgical\"  # Dataset name on HuggingFace\n",
        "REPO_ID = f\"{HF_USERNAME}/{DATASET_NAME}\"\n",
        "\n",
        "print(f\"\ud83d\udce6 Dataset will be uploaded to: https://huggingface.co/datasets/{REPO_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download PitVQA Dataset\n",
        "\n",
        "Downloading from UCL Research Data Repository.  \n",
        "Source: https://doi.org/10.5522/04/27004666"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udccd Running in: Local environment\n",
            "\ud83d\udcc2 Download directory: /Users/matheusrech/Downloads/pitvqa-surgical-workflow/notebooks/data/pitvqa_download\n",
            "\ud83d\udcc2 Data directory: /Users/matheusrech/Downloads/pitvqa-surgical-workflow/notebooks/data/pitvqa_data\n",
            "\n",
            "\ud83d\udce5 Starting download... This may take 30-60 minutes.\n",
            "   (Dataset is ~7.56 GB total)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "\n",
        "# Detect if running in Colab or locally\n",
        "import importlib.util\n",
        "try:\n",
        "    IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
        "except (ModuleNotFoundError, ValueError):\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    BASE_DIR = \"/content\"\n",
        "else:\n",
        "    BASE_DIR = os.path.join(os.getcwd(), \"data\")\n",
        "\n",
        "# Create directories\n",
        "DOWNLOAD_DIR = os.path.join(BASE_DIR, \"pitvqa_download\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"pitvqa_data\")\n",
        "\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\ud83d\udccd Running in: {'Google Colab' if IN_COLAB else 'Local environment'}\")\n",
        "print(f\"\ud83d\udcc2 Download directory: {DOWNLOAD_DIR}\")\n",
        "print(f\"\ud83d\udcc2 Data directory: {DATA_DIR}\\n\")\n",
        "\n",
        "# Download URLs from UCL RDR\n",
        "DOWNLOAD_URLS = {\n",
        "    \"videos\": \"https://rdr.ucl.ac.uk/ndownloader/files/49158880\",\n",
        "    \"annotations\": \"https://rdr.ucl.ac.uk/ndownloader/files/49228108\",\n",
        "    \"frame_annotations\": \"https://rdr.ucl.ac.uk/ndownloader/files/49228111\"\n",
        "}\n",
        "\n",
        "print(\"\ud83d\udce5 Starting download... This may take 30-60 minutes.\")\n",
        "print(\"   (Dataset is ~7.56 GB total)\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udce5 Downloading videos.zip...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|\u2588\u2588\u2588       | 2.50G/8.11G [2:40:14<3:47:40, 411kiB/s]   "
          ]
        }
      ],
      "source": [
        "# Download with progress bar\n",
        "def download_with_progress(url, filename):\n",
        "    \"\"\"Download file with progress bar.\"\"\"\n",
        "    filepath = os.path.join(DOWNLOAD_DIR, filename)\n",
        "    \n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"\u23ed\ufe0f  {filename} already exists, skipping...\")\n",
        "        return filepath\n",
        "    \n",
        "    print(f\"\ud83d\udce5 Downloading {filename}...\")\n",
        "    \n",
        "    # Get file size\n",
        "    response = urllib.request.urlopen(url)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    \n",
        "    # Download with progress\n",
        "    block_size = 8192\n",
        "    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "    \n",
        "    with open(filepath, 'wb') as f:\n",
        "        while True:\n",
        "            buffer = response.read(block_size)\n",
        "            if not buffer:\n",
        "                break\n",
        "            f.write(buffer)\n",
        "            progress_bar.update(len(buffer))\n",
        "    \n",
        "    progress_bar.close()\n",
        "    print(f\"\u2705 Downloaded: {filename}\")\n",
        "    return filepath\n",
        "\n",
        "# Download all files\n",
        "downloaded_files = {}\n",
        "for name, url in DOWNLOAD_URLS.items():\n",
        "    downloaded_files[name] = download_with_progress(url, f\"{name}.zip\")\n",
        "\n",
        "print(\"\\n\u2705 All downloads complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check downloaded files\n",
        "import subprocess\n",
        "result = subprocess.run(['ls', '-lh', DOWNLOAD_DIR], capture_output=True, text=True)\n",
        "print(result.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract & Organize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Extract all zip files\n",
        "print(\"\ud83d\udce6 Extracting files...\\n\")\n",
        "\n",
        "for name, filepath in downloaded_files.items():\n",
        "    if filepath and os.path.exists(filepath):\n",
        "        extract_dir = os.path.join(DATA_DIR, name)\n",
        "        os.makedirs(extract_dir, exist_ok=True)\n",
        "        \n",
        "        print(f\"\ud83d\udcc2 Extracting {name}...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "            print(f\"   \u2705 Extracted to {extract_dir}\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"   \u26a0\ufe0f  {filepath} is not a zip file, copying directly...\")\n",
        "            shutil.copy(filepath, extract_dir)\n",
        "\n",
        "print(\"\\n\u2705 Extraction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the extracted structure\n",
        "import subprocess\n",
        "result = subprocess.run(['find', DATA_DIR, '-type', 'f'], capture_output=True, text=True)\n",
        "files = result.stdout.strip().split('\\n')\n",
        "print('\\n'.join(files[:50]))\n",
        "print(\"\\n...\")\n",
        "print(f\"{len(files)} total files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check disk usage\n",
        "import subprocess\n",
        "try:\n",
        "    result = subprocess.run(['du', '-sh', DATA_DIR + '/*'], capture_output=True, text=True, shell=True)\n",
        "    print(result.stdout)\n",
        "except:\n",
        "    pass\n",
        "result = subprocess.run(['du', '-sh', DATA_DIR], capture_output=True, text=True)\n",
        "print(result.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create HuggingFace Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Find annotation files\n",
        "data_path = Path(DATA_DIR)\n",
        "\n",
        "# List all JSON and CSV files\n",
        "json_files = list(data_path.rglob(\"*.json\"))\n",
        "csv_files = list(data_path.rglob(\"*.csv\"))\n",
        "\n",
        "print(f\"Found {len(json_files)} JSON files\")\n",
        "print(f\"Found {len(csv_files)} CSV files\")\n",
        "\n",
        "# Show first few\n",
        "print(\"\\nJSON files:\")\n",
        "for f in json_files[:5]:\n",
        "    print(f\"  {f}\")\n",
        "    \n",
        "print(\"\\nCSV files:\")\n",
        "for f in csv_files[:5]:\n",
        "    print(f\"  {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and inspect annotation structure\n",
        "if json_files:\n",
        "    sample_json = json_files[0]\n",
        "    print(f\"Sample JSON: {sample_json}\")\n",
        "    with open(sample_json, 'r') as f:\n",
        "        sample_data = json.load(f)\n",
        "    \n",
        "    if isinstance(sample_data, list):\n",
        "        print(f\"\\nType: List with {len(sample_data)} items\")\n",
        "        print(f\"First item keys: {sample_data[0].keys() if sample_data else 'empty'}\")\n",
        "        if sample_data:\n",
        "            print(f\"\\nSample item:\")\n",
        "            print(json.dumps(sample_data[0], indent=2)[:500])\n",
        "    elif isinstance(sample_data, dict):\n",
        "        print(f\"\\nType: Dict with keys: {sample_data.keys()}\")\n",
        "        print(f\"\\nSample:\")\n",
        "        print(json.dumps(sample_data, indent=2)[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a README for the dataset\n",
        "readme_content = '''---\n",
        "license: cc-by-nc-nd-4.0\n",
        "task_categories:\n",
        "  - visual-question-answering\n",
        "  - video-classification\n",
        "language:\n",
        "  - en\n",
        "tags:\n",
        "  - medical\n",
        "  - surgical\n",
        "  - pituitary-surgery\n",
        "  - vqa\n",
        "  - endoscopic\n",
        "  - neurosurgery\n",
        "size_categories:\n",
        "  - 100K<n<1M\n",
        "---\n",
        "\n",
        "# PitVQA: Visual Question Answering in Pituitary Surgery\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "PitVQA is a dataset for Visual Question Answering (VQA) in endoscopic pituitary surgery.\n",
        "\n",
        "### Dataset Summary\n",
        "\n",
        "- **25 videos** of endoscopic pituitary surgeries\n",
        "- **109,173 frames** extracted at 1 fps\n",
        "- **884,242 question-answer pairs** (~8 per frame)\n",
        "- **59 annotation classes**:\n",
        "  - 4 surgical phases\n",
        "  - 15 surgical steps\n",
        "  - 18 surgical instruments\n",
        "  - 3 instrument presence variations\n",
        "  - 5 instrument positions\n",
        "  - 14 operation notes\n",
        "\n",
        "### Source\n",
        "\n",
        "Original dataset from UCL Research Data Repository:  \n",
        "https://doi.org/10.5522/04/27004666\n",
        "\n",
        "Collected at the National Hospital of Neurology and Neurosurgery, London, UK.\n",
        "\n",
        "### Citation\n",
        "\n",
        "```bibtex\n",
        "@article{hoque2024pitvqa,\n",
        "  title={PitVQA: Image-grounded Text Embedding LLM for Visual Question Answering in Pituitary Surgery},\n",
        "  author={Hoque, Mobarak and Clarkson, Matt and Bano, Sophia and Stoyanov, Danail and Marcus, Hani},\n",
        "  journal={arXiv preprint arXiv:2405.13949},\n",
        "  year={2024}\n",
        "}\n",
        "```\n",
        "\n",
        "### License\n",
        "\n",
        "CC BY-NC-ND 4.0 (Non-commercial, No Derivatives)\n",
        "\n",
        "### Related Resources\n",
        "\n",
        "- [PitVQA Paper (arXiv)](https://arxiv.org/abs/2405.13949)\n",
        "- [PitVQA GitHub](https://github.com/mobarakol/PitVQA)\n",
        "- [MICCAI PitVis Challenge](https://www.synapse.org/Synapse:syn51232283)\n",
        "\n",
        "---\n",
        "\n",
        "*Uploaded to HuggingFace Hub for research purposes.*\n",
        "'''\n",
        "\n",
        "readme_path = os.path.join(DATA_DIR, \"README.md\")\n",
        "with open(readme_path, \"w\") as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\"\u2705 Created README.md for dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Upload to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Create the dataset repository\n",
        "try:\n",
        "    create_repo(\n",
        "        repo_id=REPO_ID,\n",
        "        repo_type=\"dataset\",\n",
        "        private=False,\n",
        "        exist_ok=True\n",
        "    )\n",
        "    print(f\"\u2705 Repository created/exists: https://huggingface.co/datasets/{REPO_ID}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Note: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the entire dataset folder\n",
        "print(f\"\ud83d\udce4 Uploading dataset to HuggingFace Hub...\")\n",
        "print(f\"   This may take 30-60 minutes for ~7.5GB\")\n",
        "print(f\"   Repository: https://huggingface.co/datasets/{REPO_ID}\\n\")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=DATA_DIR,\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    commit_message=\"Upload PitVQA dataset from UCL RDR\",\n",
        "    ignore_patterns=[\"*.zip\", \"__MACOSX/*\", \".DS_Store\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83c\udf89 Upload complete!\")\n",
        "print(f\"\\n\ud83d\udce6 Dataset available at: https://huggingface.co/datasets/{REPO_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Verify Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List files in the uploaded repository\n",
        "from huggingface_hub import list_repo_files\n",
        "\n",
        "files = list_repo_files(REPO_ID, repo_type=\"dataset\")\n",
        "print(f\"\ud83d\udcc2 Files in repository ({len(files)} total):\\n\")\n",
        "for f in files[:20]:\n",
        "    print(f\"  {f}\")\n",
        "if len(files) > 20:\n",
        "    print(f\"  ... and {len(files) - 20} more files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test loading the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"\ud83d\udd04 Testing dataset loading...\")\n",
        "try:\n",
        "    # Try to load (this verifies the upload worked)\n",
        "    ds = load_dataset(REPO_ID, trust_remote_code=True)\n",
        "    print(f\"\\n\u2705 Dataset loads successfully!\")\n",
        "    print(ds)\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u26a0\ufe0f Note: {e}\")\n",
        "    print(\"Dataset uploaded but may need manual dataset script.\")\n",
        "    print(f\"Files are accessible at: https://huggingface.co/datasets/{REPO_ID}/tree/main\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cleanup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up downloaded files to free disk space\n",
        "cleanup = input(\"Delete local files to free disk space? (y/n): \")\n",
        "\n",
        "if cleanup.lower() == 'y':\n",
        "    import shutil\n",
        "    shutil.rmtree(DOWNLOAD_DIR, ignore_errors=True)\n",
        "    shutil.rmtree(DATA_DIR, ignore_errors=True)\n",
        "    print(\"\u2705 Cleaned up local files\")\n",
        "else:\n",
        "    print(\"\u23ed\ufe0f Keeping local files\")\n",
        "\n",
        "# Check disk usage\n",
        "if IN_COLAB:\n",
        "    import subprocess\n",
        "    result = subprocess.run(['df', '-h', '/content'], capture_output=True, text=True)\n",
        "    print(result.stdout)\n",
        "else:\n",
        "    import shutil\n",
        "    total, used, free = shutil.disk_usage(BASE_DIR)\n",
        "    print(f\"\\n\ud83d\udcbe Disk usage:\")\n",
        "    print(f\"   Total: {total // (1024**3)} GB\")\n",
        "    print(f\"   Used: {used // (1024**3)} GB\")\n",
        "    print(f\"   Free: {free // (1024**3)} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \u2705 Done!\n",
        "\n",
        "Your PitVQA dataset is now on HuggingFace Hub!\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **View your dataset**: https://huggingface.co/datasets/{REPO_ID}\n",
        "2. **Use in training**:\n",
        "   ```python\n",
        "   from datasets import load_dataset\n",
        "   ds = load_dataset(\"matheus-rech/pitvqa-surgical\")\n",
        "   ```\n",
        "3. **Stream without downloading**:\n",
        "   ```python\n",
        "   ds = load_dataset(\"matheus-rech/pitvqa-surgical\", streaming=True)\n",
        "   ```\n",
        "\n",
        "### Project Links\n",
        "\n",
        "- GitHub: https://github.com/matheus-rech/pitvqa-surgical-workflow\n",
        "- Dataset: https://huggingface.co/datasets/matheus-rech/pitvqa-surgical\n",
        "\n",
        "---\n",
        "\n",
        "*Generated with Claude Code for MICCAI 2026 project*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "accelerator": "NONE",
      "colab_type": "notebook",
      "name": "01_upload_pitvqa_to_huggingface.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}